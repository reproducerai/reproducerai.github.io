{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# --------------------------\n",
    "# --- FIX SEEDS & DEVICE ---\n",
    "# --------------------------\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# --- DATASET ---\n",
    "# --------------------------\n",
    "class MotifDatasetEmbed8(Dataset):\n",
    "    def __init__(self, num_samples=5000, seq_len=100, embed_dim=8, motif_len=3, num_motifs=5, seed=SEED):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.motif_len = motif_len\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Create several motif variants\n",
    "        self.motifs = [self.rng.normal(loc=1.0, scale=0.5, size=(motif_len, embed_dim)) for _ in range(num_motifs)]\n",
    "        self.pos_encoding = self.get_positional_encoding(seq_len, embed_dim)\n",
    "\n",
    "        self.data, self.labels = self._generate_dataset()\n",
    "\n",
    "    def get_positional_encoding(self, seq_len, embed_dim):\n",
    "        pe = np.zeros((seq_len, embed_dim))\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, embed_dim, 2):\n",
    "                pe[pos, i] = np.sin(pos / (10000 ** ((2 * i)/embed_dim)))\n",
    "                if i + 1 < embed_dim:\n",
    "                    pe[pos, i+1] = np.cos(pos / (10000 ** ((2 * i)/embed_dim)))\n",
    "        return torch.tensor(pe, dtype=torch.float32)\n",
    "\n",
    "    def _generate_dataset(self):\n",
    "        data, labels = [], []\n",
    "        for _ in range(self.num_samples):\n",
    "            seq = self.rng.normal(loc=0.0, scale=1.0, size=(self.seq_len, self.embed_dim))\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                motif = random.choice(self.motifs)\n",
    "                start = random.randint(0, self.seq_len - self.motif_len)\n",
    "                seq[start:start+self.motif_len] = motif + self.rng.normal(0, 0.1, motif.shape)\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "\n",
    "            seq += self.pos_encoding.numpy()\n",
    "            data.append(seq)\n",
    "            labels.append(label)\n",
    "\n",
    "        return torch.tensor(np.stack(data), dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# --------------------------\n",
    "# --- DATALOADERS ---\n",
    "# --------------------------\n",
    "train_size, val_size, test_size = 5000, 1000, 1000\n",
    "full_dataset = MotifDatasetEmbed8(num_samples=train_size+val_size, seed=SEED)\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "test_dataset = MotifDatasetEmbed8(num_samples=test_size, seed=SEED+1)  # different seed for test set\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# --------------------------\n",
    "# --- MODELS ---\n",
    "# --------------------------\n",
    "class MLPClassifierBinary(nn.Module):\n",
    "    def __init__(self, seq_len=100, embed_dim=8, hidden_dim=64, seed=SEED):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(seq_len * embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.flatten(x))\n",
    "\n",
    "class TransformerClassifierBinary(nn.Module):\n",
    "    def __init__(self, embed_dim=8, num_heads=1, num_layers=1, seq_len=100, seed=SEED):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# --------------------------\n",
    "# --- TRAINING FUNCTION ---\n",
    "# --------------------------\n",
    "def train(model, train_loader, val_loader, test_loader, epochs=10):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_threshold = 0.5\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_batch), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_outputs, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device).float().unsqueeze(1)\n",
    "                val_outputs.append(model(X_batch))\n",
    "                val_labels.append(y_batch)\n",
    "        val_outputs = torch.cat(val_outputs)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "\n",
    "        # Threshold search\n",
    "        thresholds = torch.linspace(0.1, 0.9, steps=81)\n",
    "        val_accs = [( (torch.sigmoid(val_outputs) > t).long() == val_labels.long() ).float().mean().item() for t in thresholds]\n",
    "        best_idx = torch.tensor(val_accs).argmax()\n",
    "        best_epoch_acc = val_accs[best_idx]\n",
    "        best_epoch_threshold = thresholds[best_idx].item()\n",
    "\n",
    "        if best_epoch_acc > best_val_acc:\n",
    "            best_val_acc = best_epoch_acc\n",
    "            best_threshold = best_epoch_threshold\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f\"Epoch {epoch}: Val Accuracy = {best_epoch_acc:.3f}, Best Threshold = {best_epoch_threshold:.2f}\")\n",
    "\n",
    "    # Test\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).float().unsqueeze(1)\n",
    "            preds = (torch.sigmoid(model(X_batch)) > best_threshold).long()\n",
    "            correct += (preds.squeeze() == y_batch.squeeze().long()).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    test_acc = correct / total\n",
    "    print(f\"\\nTest Accuracy = {test_acc:.3f} using threshold {best_threshold:.2f}\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# --- RUN TRAINING ---\n",
    "# --------------------------\n",
    "mlp = MLPClassifierBinary(seq_len=100, embed_dim=8, hidden_dim=64)\n",
    "transformer = TransformerClassifierBinary(embed_dim=8, num_heads=1, num_layers=1, seq_len=100)\n",
    "\n",
    "print(\"Training MLP:\")\n",
    "train(mlp, train_loader, val_loader, test_loader, epochs=10)\n",
    "\n",
    "print(\"Training Transformer:\")\n",
    "train(transformer, train_loader, val_loader, test_loader, epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
