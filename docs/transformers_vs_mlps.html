<h1>Experiment: Transformers vs MLPs on Sequence Data</h1>

<h2>Objective</h2>
<p>
Test how different AI architectures handle sequence data, using a simple synthetic dataset with embedded motifs. 
The goal is to see how well models can detect patterns when sequences vary in length and complexity.
</p>

<h2>Setup</h2>
<p><strong>Dataset:</strong> Synthetic sequences of length 10 (later extended to 100), each position represented by an 8-dimensional embedding. Some sequences contain a small motif, others don’t. Positional encoding is added to each sequence.</p>

<p><strong>Models:</strong></p>
<ul>
  <li><strong>MLP (Multi-Layer Perceptron):</strong> Flattened input, one hidden layer with 64 units.</li>
  <li><strong>Transformer:</strong> Single-layer Transformer with 1 attention head; sequence structure preserved.</li>
</ul>

<p><strong>Training:</strong> Binary classification to detect the motif. Optimized with Adam and BCEWithLogitsLoss. Validation threshold tuned for best accuracy.</p>

<h2>Findings</h2>
<ul>
  <li>For short sequences (seq_len=10), both MLP and Transformer achieve high accuracy (~97–98%).</li>
  <li>For longer sequences (seq_len=100), the Transformer significantly outperforms the MLP:
    <ul>
      <li>MLP struggles (~55–60% accuracy) because flattening destroys positional information and the motif signal is diluted.</li>
      <li>Transformer maintains strong performance (~80–85%) by attending to motif positions across the sequence.</li>
    </ul>
  </li>
</ul>

<h2>Conclusion</h2>
<ul>
  <li>MLPs work well for short sequences but fail as sequences grow longer and sparser.</li>
  <li>Transformers handle long sequences effectively by capturing sequence structure and focusing on relevant positions.</li>
  <li>This simple example illustrates why attention-based architectures are preferred for sequence data.</li>
</ul>
