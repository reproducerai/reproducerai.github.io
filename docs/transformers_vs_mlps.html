<h1>Experiment: Transformers vs MLPs on Sequence Data</h1>

<h2>Objective</h2>
<p>
Compare an MLP and a Transformer for sequence classification
</p>

<h2>Setup</h2>
<p><strong>Dataset:</strong>
Sequences of length 10, 20, 30, …, up to 100, each consisting of 8-dimensional vectors drawn from a normal distribution (mean = 0, std = 1).
    About half of the sequences (random 50%) contain 3 consecutive embeddings from a different normal distribution
    (mean = 1, std = 0.5 + tiny noise) inserted at a random position. Sequences containing this 3-vector pattern are labeled 1;
    those without it are labeled 0.

<img src="images/1.1 Transformers vs MLP.png" alt="Dataset illustration: sequences with and without pattern" width="800">



<p><strong>Models:</strong></p>
<ul>
  <li><strong>MLP (Multi-Layer Perceptron):</strong> Flattened input, one or more hidden layers, single output.</li>
  <li><strong>Transformer:</strong> Single-layer Transformer with one attention head, positional encoding, single output.</li>
</ul>

<img src="images/1.2 Transformers vs MLP new.png" alt="MLP architecture" width="1200">


<p><strong>Training:</strong> Binary classification to detect the pattern. Optimized with Adam and BCEWithLogitsLoss. Validation threshold tuned for best accuracy.</p>

<h2>Findings</h2>
<ul>
  <li>For short sequences (seq_len=10), both MLP and Transformer achieve high accuracy (~97–98%).</li>
  <li>For longer sequences (seq_len=100), the Transformer significantly outperforms the MLP:
    <ul>
      <li>MLP struggles (~55–60% accuracy) because flattening destroys positional information and the pattern signal is diluted.</li>
      <li>Transformer maintains strong performance (~80–85%) by attending to pattern positions across the sequence.</li>
    </ul>
  </li>
</ul>

<h2>Conclusion</h2>
<ul>
  <li>MLPs work well for short sequences but fail as sequences grow longer and sparser.</li>
  <li>Transformers handle long sequences effectively by capturing sequence structure and focusing on relevant positions.</li>
  <li>This simple example illustrates why attention-based architectures are preferred for sequence data.</li>
</ul>
