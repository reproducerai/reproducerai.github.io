<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>MLP vs Self-Attention+MLP Experiment</title>
  <style>
    /* Base font size and line height */
    body {
      font-size: 18px;
      line-height: 1.6;
      font-family: Arial, sans-serif;
      margin: 20px;
      color: #222;
      background-color: #f9f9f9;
    }

    h1 {
      font-size: 2.5em;
      margin-bottom: 0.5em;
    }

    h2 {
      font-size: 2em;
      margin-top: 1.5em;
      margin-bottom: 0.5em;
    }

    h3 {
      font-size: 1.5em;
      margin-top: 1.2em;
      margin-bottom: 0.3em;
    }

    p {
      margin-bottom: 1em;
    }

    ul {
      margin-bottom: 1em;
    }

    li {
      margin-bottom: 0.5em;
    }

    img {
      display: block;
      margin: 1em auto;
      max-width: 100%;
      height: auto;
    }

    strong {
      color: #444;
    }

    .dataset-info ul {
      list-style-type: none;
      padding-left: 0;
    }

    .dataset-info ul li {
      margin-bottom: 0.3em;
    }
  </style>
</head>
<body>

<a href="https://colab.research.google.com/drive/1-3PyxGslKZBU36QRVoTZ4j9exfWYso0V?usp=sharing" target="_blank" style="text-decoration:none;">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
</a>

<h1>Experiment: MLP vs Self-Attention+MLP on Sequence Data</h1>

<h2>Objective</h2>
<p>Compare an MLP and Self-Attention+MLP for a simple sequence classification task.</p>

<h2>Dataset</h2>
<ul class="dataset-info">
  <li>
    <p><strong>Definition</strong></p>
      We generate sequences with length of <i>N</i>.
      Each element in a sequence is an 8-dimensional vector sampled from a standard normal distribution (mean = 0, std = 1).

      For about half of the sequences (randomly chosen 50%), we insert three consecutive vectors drawn from a different normal distribution (mean = 1, std = 0.5 + small noise) at a random position within the sequence.
      <ul>
        <li>Sequences with this 3-vector pattern are labeled 1</li>
        <li>Sequences without it are labeled 0</li>
      </ul>
  </li>
  <li>
    <p><strong>Shape and Distribution</strong></p>
    <ul>
      <li>Train: Shape 10000 × <i>N</i> × 8, 1/0 Count: (4969, 5031)</li>
      <li>Validation: Shape 1500 × <i>N</i> × 8, 1/0 Count: (733, 767)</li>
      <li>Test: Shape 1500 × <i>N</i> × 8, 1/0 Count: (782, 718)</li>
    </ul>
  </li>
</ul>

<img src="images/1.1 Transformers vs MLP new.png" alt="Dataset illustration">

<h2>Models</h2>
<ul>
  <li><strong>MLP (Multi-Layer Perceptron)</strong>: 53,761 trainable parameters</li>
  <li><strong>Self-Attention + MLP</strong>: 54,049 trainable parameters</li>
</ul>

<img src="images/1.2 Transformers vs MLP.png" alt="MLP architecture">

<h2>Training</h2>
<ul>
  <li>Task: Binary classification.</li>
  <li>Optimizer: Adam, initial learning rate = 1e-4, all other settings default.</li>
  <li>Loss: BCEWithLogitsLoss, default settings.</li>
  <li>Training duration: 25 epochs.</li>
</ul>

<h2>Findings</h2>
<ul>
  <li>For short sequences, both MLP and Self-Attention+MLP achieve high accuracy.</li>
  <li>For longer sequences, Self-Attention+MLP outperforms MLP significantly.</li>
</ul>

<img src="images/1.3 Transformers vs MLP.png" alt="Results" width="800">

<h2>Conclusion and Code</h2>
<ul>
  <li>MLPs work well for short sequences but fail as sequences grow longer.</li>
  <li>Self-Attention+MLP handles long sequences effectively by capturing sequence structure and focusing on relevant positions.</li>
  <li>
    <strong>Code:</strong>
    <a href="https://colab.research.google.com/drive/1-3PyxGslKZBU36QRVoTZ4j9exfWYso0V?usp=sharing" target="_blank" style="text-decoration:none;">
      Open in Google Colab
    </a>
  </li>
</ul>

</body>
</html>
