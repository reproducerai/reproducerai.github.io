<h1>Experiment: Transformers vs MLPs on Sequence Data</h1>

<h2>Objective</h2>
<p>
Compare an MLP and a Transformer for sequence classification
</p>

<h2>Setup</h2>
<p><strong>Dataset:</strong>
Sequences of length 10, 20, 30, â€¦, up to 100, each consisting of 8-dimensional vectors drawn from a normal distribution (mean = 0, std = 1).
    About half of the sequences (random 50%) contain 3 consecutive embeddings from a different normal distribution
    (mean = 1, std = 0.5 + tiny noise) inserted at a random position. Sequences containing this 3-vector pattern are labeled 1;
    those without it are labeled 0.

<img src="images/1.1 Transformers vs MLP.png" alt="Dataset illustration: sequences with and without pattern" width="800">



<p><strong>Models:</strong></p>
<ul>
  <li><strong>MLP (Multi-Layer Perceptron)</strong></li>
  <li><strong>Self-Attention + MLP (Multi-Layer Perceptron)</strong></li>
</ul>

<img src="images/1.2 Transformers vs MLP new.png" alt="MLP architecture" width="1200">


<p><strong>Training:</strong> Binary classification to detect the pattern. Optimized with Adam and BCEWithLogitsLoss. Validation threshold tuned for best accuracy.</p>

<h2>Findings</h2>
<ul>
  <li>For short sequences (seq_len=10), both MLP and Self-Attention+MLP achieve high accuracy.</li>
  <li>For longer sequences (seq_len=100), the Self-Attention+MLP outperforms the MLP.</li>
</ul>

<h2>Conclusion</h2>
<ul>
  <li>MLPs work well for short sequences but fail as sequences grow longer and sparser.</li>
  <li>Self-Attention+MLP  handle long sequences effectively by capturing sequence structure and focusing on relevant positions.</li>
</ul>
