<h1>Experiment: Transformers vs MLPs on Sequence Data</h1>

<h2>Objective</h2>
<p>
Compare an MLP and a Transformer for sequence classification
</p>

<h2>Dataset</h2>
<ul>
  <li>
    <p><strong>Definition</strong></p>
    Sequences of length 10, 20, 30, â€¦, up to 100, each consisting of 8-dimensional vectors drawn from a normal distribution (mean = 0, std = 1).
    About half of the sequences (random 50%) contain 3 consecutive embeddings from a different normal distribution
    (mean = 1, std = 0.5 + tiny noise) inserted at a random position. Sequences containing this 3-vector pattern are labeled 1;
    those without it are labeled 0.
  </li>
  <li>
    <p><strong>Distribution</strong></p>
    <div>Train: Shape 10000xsequence_lengthx8, Positive/Negative: (4969, 5031)</div>
    <div>Validation: Shape 1500xsequence_lengthx8, Positive/Negative: (733, 767)</div>
    <div>Test: Shape 1500xsequence_lengthx8, Positive/Negative: (782, 718)</div>
  </li>
</ul>


Positive/Negative Samples: (733, 767)
Positive/Negative Samples: (782, 718)
Training MLP:

<img src="images/1.1 Transformers vs MLP.png" alt="Dataset illustration: sequences with and without pattern" width="800">


<h2>Models</h2>
<ul>
  <li><strong>MLP (Multi-Layer Perceptron): </strong>(53761 trainable paremeters)</li>
  <li><strong>Self-Attention + MLP (Multi-Layer Perceptron): </strong>(54049 trainable parameters)</li>
</ul>

<img src="images/1.2 Transformers vs MLP new.png" alt="MLP architecture" width="1200">


<h2>Training</h2>
<p>
  Binary classification to detect the pattern. Optimized with Adam and BCEWithLogitsLoss. Validation threshold tuned for best accuracy.
</p>

<h2>Findings</h2>
<ul>
  <li>For short sequences (seq_len=10), both MLP and Self-Attention+MLP achieve high accuracy.</li>
  <li>For longer sequences (seq_len=100), the Self-Attention+MLP outperforms the MLP.</li>
</ul>

<h2>Conclusion</h2>
<ul>
  <li>MLPs work well for short sequences but fail as sequences grow longer and sparser.</li>
  <li>Self-Attention+MLP  handle long sequences effectively by capturing sequence structure and focusing on relevant positions.</li>
</ul>
