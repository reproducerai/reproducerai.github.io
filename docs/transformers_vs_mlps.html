<h1>Experiment: Transformers vs MLPs on Sequence Data</h1>

<h2>Objective</h2>
<p>
Compare an MLP and a Transformer for sequence classification, focusing on how well each detects patterns in sequences of different lengths.
</p>

<h2>Setup</h2>
<p><strong>Dataset:</strong> Synthetic sequences of varying lengths (10 to 100, in steps of 10), each position represented by an 8-dimensional embedding. 
    Some sequences contain a small pattern—these are labeled <strong>1</strong>; sequences without the pattern are labeled <strong>0</strong>.</p>

<p><strong>Models:</strong></p>
<ul>
  <li><strong>MLP (Multi-Layer Perceptron):</strong> Flattened input, one hidden layer with 64 units.</li>
  <li><strong>Transformer:</strong> Single-layer Transformer with 1 attention head; sequence structure preserved.</li>
</ul>

<p><strong>Training:</strong> Binary classification to detect the motif. Optimized with Adam and BCEWithLogitsLoss. Validation threshold tuned for best accuracy.</p>

<h2>Findings</h2>
<ul>
  <li>For short sequences (seq_len=10), both MLP and Transformer achieve high accuracy (~97–98%).</li>
  <li>For longer sequences (seq_len=100), the Transformer significantly outperforms the MLP:
    <ul>
      <li>MLP struggles (~55–60% accuracy) because flattening destroys positional information and the motif signal is diluted.</li>
      <li>Transformer maintains strong performance (~80–85%) by attending to motif positions across the sequence.</li>
    </ul>
  </li>
</ul>

<h2>Conclusion</h2>
<ul>
  <li>MLPs work well for short sequences but fail as sequences grow longer and sparser.</li>
  <li>Transformers handle long sequences effectively by capturing sequence structure and focusing on relevant positions.</li>
  <li>This simple example illustrates why attention-based architectures are preferred for sequence data.</li>
</ul>
