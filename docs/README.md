### Purpose
Creating minimal empirical examples to appreciate how different AI architectures, learning strategies, and dataset designs influence model performance.

---

### Connect
<a href="https://www.linkedin.com/in/minhaj-uddin-ansari-040573160/" target="_blank">LinkedIn</a>,
<a href="https://x.com/minhajansari_" target="_blank">Twitter/X</a>,
<a href="mailto:reproducerai@gmail.com">reproducerai@gmail.com</a>

---

### Journey

<div style="display: flex; flex-direction: column; gap: 10px;">
<a href="transformers_vs_mlps.html" target="_blank">
  MLP vs Self-Attention+MLP: A Minimal Example
</a>
<a href="transformers_pe_vs_no_pe.html" target="_blank">
  Positional Encoding vs No Positional Encoding: A Minimal Example
</a>
<a href="shared_vs_separate.html" target="_blank">
  Separate 
  W<sub>Q</sub>, W<sub>K</sub>, W<sub>V</sub> vs Shared 
  W<sub>Q</sub> = W<sub>K</sub> = W<sub>V</sub> Projection Matrices
</a>
</div>


